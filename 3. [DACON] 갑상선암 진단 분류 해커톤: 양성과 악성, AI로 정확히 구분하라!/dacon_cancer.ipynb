{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598a1dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Import\n",
    "# ==================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import optuna\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2094370f",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd5b2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# EDA\n",
    "# ==================================================\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# `dtype` & Missing Values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0164a90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categrical Variables VS 'Cancer'\n",
    "eda_cat_col = df.copy().select_dtypes(exclude = 'number').columns\n",
    "for c in eda_cat_col:\n",
    "    if c != 'ID':\n",
    "        ct = pd.crosstab(df[c], df['Cancer'], margins = True)\n",
    "        ct['Odds'] = ct[1] / ct[0]\n",
    "        ct['WOE'] = np.log(ct['Odds'] / ct.loc['All', 'Odds'])\n",
    "        print('=' * 40)\n",
    "        print(ct)\n",
    "        print('=' * 40, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d98fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Variables VS 'Cancer'\n",
    "eda_num_col = df.copy().select_dtypes(include='number').columns.to_list()\n",
    "n = len(eda_num_col)\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, c in enumerate(eda_num_col):\n",
    "    if c != 'Cancer':\n",
    "        data_0 = df[df['Cancer'] == 0][c]\n",
    "        data_1 = df[df['Cancer'] == 1][c]\n",
    "\n",
    "        axes[i].boxplot([data_0, data_1], tick_labels=['0', '1'])\n",
    "        axes[i].set_title(f'{c}')\n",
    "        axes[i].set_xlabel('Cancer')\n",
    "        axes[i].set_ylabel(c)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e9ba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binned Numerical Variables VS 'Cancer'\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "axes = axes.flatten()\n",
    "k = int(np.log2(len(df)) + 1)\n",
    "bins = df.copy()[eda_num_col]\n",
    "for i, c in enumerate(eda_num_col):\n",
    "    if c != 'Cancer':\n",
    "        bins[f'{c}_bin'] = pd.cut(df[c], bins = k, labels = False)\n",
    "        ct = pd.crosstab(bins[f'{c}_bin'], bins['Cancer'], margins = True)\n",
    "        ct['Odds'] = ct[1] / ct[0]\n",
    "        ct['WOE'] = np.log(ct['Odds'] / ct.loc['All', 'Odds'])\n",
    "        ct = ct.drop('All')\n",
    "\n",
    "        axes[i].plot(ct.index.astype(int), ct['WOE'])\n",
    "        axes[i].set_title(c)\n",
    "        axes[i].set_xlabel('Bin')\n",
    "        axes[i].set_ylabel('WOE')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097733e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustered Numerical Variables VS 'Cancer'\n",
    "clusters = df.copy()[eda_num_col]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "kmeans = KMeans(n_clusters = 16, random_state = 42) # Based on Silhouette Score\n",
    "\n",
    "clusters[[\n",
    "    'Age', 'Nodule_Size','TSH_Result', 'T4_Result', 'T3_Result'\n",
    "]] = scaler.fit_transform(clusters[[\n",
    "    'Age', 'Nodule_Size','TSH_Result', 'T4_Result', 'T3_Result'\n",
    "]])\n",
    "clusters['Cluster'] = kmeans.fit_predict(clusters[[\n",
    "    'Age', 'Nodule_Size','TSH_Result', 'T4_Result', 'T3_Result'\n",
    "]])\n",
    "ct = pd.crosstab(clusters['Cluster'], clusters['Cancer'], margins = True)\n",
    "ct['Odds'] = ct[1] / ct[0]\n",
    "ct['WOE'] = np.log(ct['Odds'] / ct.loc['All', 'Odds'])\n",
    "ct = ct.drop('All')\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.bar(ct.index.astype(int), ct['WOE'])\n",
    "plt.title('Cluster')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('WOE')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0976fcd",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbcf444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Preprocessing\n",
    "# ==================================================\n",
    "\n",
    "# Clustered & Binned Numerical Variables\n",
    "def AddVariables(X):\n",
    "    train = pd.read_csv('train.csv')\n",
    "    cols_to_modify = [\n",
    "        'Age', 'Nodule_Size','TSH_Result', 'T4_Result', 'T3_Result'\n",
    "    ]\n",
    "    cols_binned = [col + '_Binned' for col in cols_to_modify]\n",
    "\n",
    "    binner = KBinsDiscretizer(\n",
    "        n_bins = 17, encode = 'ordinal'\n",
    "    )\n",
    "    binner = binner.fit(train[cols_to_modify])\n",
    "    X[cols_binned] = binner.transform(X[cols_to_modify].copy())\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    cluster = KMeans(n_clusters = 16, random_state = 42)\n",
    "    train[cols_to_modify] = scaler.fit_transform(train[cols_to_modify])\n",
    "    cluster = cluster.fit(train[cols_to_modify])\n",
    "\n",
    "    X_copy = X.copy()\n",
    "    X_copy[cols_to_modify] = scaler.transform(X_copy[cols_to_modify])\n",
    "    X['Cluster'] = cluster.predict(X_copy[cols_to_modify])\n",
    "\n",
    "    X['Cluster'] = X['Cluster'].astype(object)\n",
    "    X[cols_binned] = X[cols_binned].astype(object)\n",
    "\n",
    "    return X\n",
    "\n",
    "# Custom WOE Encoder\n",
    "class WOEEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns = None, handle_unknown = 'value', unknown_value = 0.0):\n",
    "        self.columns = list(columns) if columns is not None else None\n",
    "        self.handle_unknown = handle_unknown\n",
    "        self.unknown_value = unknown_value\n",
    "        self._output_transform = 'default'\n",
    "\n",
    "    def set_output(self, *, transform = None):\n",
    "        transform = transform or 'default'\n",
    "        if transform not in ('default', 'pandas'):\n",
    "            raise ValueError(\"transform must be 'default' or 'pandas'\")\n",
    "        self._output_transform = transform\n",
    "        return self\n",
    "\n",
    "    def get_feature_names_out(self, input_features = None):\n",
    "        return list(self.columns) if self.columns is not None else list(input_features)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_df = pd.DataFrame(X)\n",
    "        if self.columns is None:\n",
    "            self.columns = X_df.columns.tolist()\n",
    "        else:\n",
    "            X_df = X_df[self.columns]\n",
    "\n",
    "        y = pd.Series(y, name='target')\n",
    "        total_pos = y.sum()\n",
    "        total_neg = len(y) - total_pos\n",
    "\n",
    "        self.woe_dict_ = {}\n",
    "        self.inv_woe_dict_ = {}\n",
    "\n",
    "        for col in self.columns:\n",
    "            grp = X_df.assign(target = y).groupby(col)['target']\n",
    "            pos = grp.sum().replace(0, 0.5)\n",
    "            neg = grp.count().sub(pos).replace(0, 0.5)\n",
    "\n",
    "            rate_pos = pos / total_pos\n",
    "            rate_neg = neg / total_neg\n",
    "            woe = np.log(rate_pos / rate_neg)\n",
    "\n",
    "            mapping = woe.to_dict()\n",
    "            self.woe_dict_[col] = mapping\n",
    "            self.inv_woe_dict_[col] = {v: k for k, v in mapping.items()}\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_df = pd.DataFrame(X)\n",
    "        X_enc = X_df[self.columns].copy()\n",
    "\n",
    "        for col in self.columns:\n",
    "            mapping = self.woe_dict_[col]\n",
    "            X_enc[col] = X_enc[col].map(mapping)\n",
    "            if self.handle_unknown == 'value':\n",
    "                X_enc[col] = X_enc[col].fillna(self.unknown_value)\n",
    "            else:\n",
    "                if X_enc[col].isnull().any():\n",
    "                    unseen = set(X_df.loc[X_enc[col].isnull(), col])\n",
    "                    raise ValueError(f\"Unseen categories {unseen} in column '{col}'\")\n",
    "\n",
    "        arr = X_enc.values\n",
    "        if self._output_transform == 'pandas':\n",
    "            return pd.DataFrame(arr, columns=self.get_feature_names_out(), index = X_df.index)\n",
    "        if arr.shape[1] == 1:\n",
    "            return arr.ravel()\n",
    "        return arr\n",
    "\n",
    "    def inverse_transform(self, X_enc):\n",
    "        arr = np.array(X_enc)\n",
    "        if arr.ndim == 1:\n",
    "            arr = arr.reshape(-1, 1)\n",
    "        if arr.shape[1] != len(self.columns):\n",
    "            raise ValueError(f\"Expected {len(self.columns)} features, got {arr.shape[1]}\")\n",
    "\n",
    "        decoded = []\n",
    "        for i, col in enumerate(self.columns):\n",
    "            inv_map = self.inv_woe_dict_[col]\n",
    "            col_vals = arr[:, i]\n",
    "            decoded.append([inv_map.get(v, None) for v in col_vals])\n",
    "        decoded = np.array(decoded).T\n",
    "        if decoded.shape[1] == 1:\n",
    "            decoded = decoded.ravel()\n",
    "        return decoded if self._output_transform == 'default' else pd.DataFrame(decoded, columns = self.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fbe8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Preprocessing: Train Data\n",
    "# ==================================================\n",
    "\n",
    "# Add Derived Variable\n",
    "train = pd.read_csv('train.csv')\n",
    "train = AddVariables(train)\n",
    "\n",
    "# Input-Output Split\n",
    "x = train.drop(['ID', 'Cancer'], axis = 1)\n",
    "y = train['Cancer']\n",
    "\n",
    "# Categorize Columns\n",
    "num_col = x.select_dtypes(include = 'number').columns.to_list()\n",
    "cat_col = x.select_dtypes(exclude = 'number').columns.to_list()\n",
    "\n",
    "# Preprocessor for Tree-Based Models\n",
    "prep_tree = ColumnTransformer([\n",
    "    ('cat', WOEEncoder(), cat_col),\n",
    "    ('num', 'passthrough', num_col)\n",
    "]).set_output(transform = 'pandas')\n",
    "\n",
    "# Preprocessor for Non-Tree-Based Models\n",
    "prep_scale = ColumnTransformer([\n",
    "    ('cat', Pipeline([\n",
    "        ('enc', WOEEncoder()),\n",
    "        ('scale', StandardScaler())\n",
    "    ]), cat_col),\n",
    "    ('num', StandardScaler(), num_col)\n",
    "])\n",
    "\n",
    "# Set CV\n",
    "cv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df88df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Preprocessing: Test Data\n",
    "# ==================================================\n",
    "\n",
    "# Add Derived Variable\n",
    "test = pd.read_csv('test.csv')\n",
    "test = AddVariables(test)\n",
    "x_test = test.drop('ID', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe51b02",
   "metadata": {},
   "source": [
    "# LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60a3343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# LGBM\n",
    "# ==================================================\n",
    "\n",
    "# Define Pipeline\n",
    "lgbm = Pipeline([\n",
    "    ('prep', prep_tree),\n",
    "    ('model', LGBMClassifier(\n",
    "        boosting_type = 'gbdt',\n",
    "        objective = 'binary',\n",
    "        is_unbalance = True,\n",
    "        metric = 'auc',\n",
    "        random_state = 42,\n",
    "        verbosity = -1, \n",
    "        device = 'gpu'\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Optuna Objective\n",
    "def lgbm_objective(trial):\n",
    "    params = {\n",
    "        'model__num_leaves': trial.suggest_int('num_leaves', 16, 256),\n",
    "        'model__max_depth': trial.suggest_int('max_depth', 4, 16),\n",
    "        'model__learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.2, log = True),\n",
    "        'model__n_estimators': trial.suggest_int('n_estimators', 300, 2000),\n",
    "        'model__min_split_gain': trial.suggest_float('min_split_gain', 0.0, 0.1),\n",
    "        'model__min_child_samples': trial.suggest_int('min_child_samples', 50, 300),\n",
    "        'model__subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'model__subsample_freq': trial.suggest_int('subsample_freq', 1, 5),\n",
    "        'model__colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'model__reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "        'model__reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0)\n",
    "    }\n",
    "\n",
    "    optuna_pipeline = clone(lgbm).set_params(**params)\n",
    "\n",
    "    scores = cross_validate(\n",
    "        optuna_pipeline, x, y,\n",
    "        scoring = 'f1',\n",
    "        cv = cv,\n",
    "        n_jobs = 3,\n",
    "        verbose = 1\n",
    "    )\n",
    "    \n",
    "    return scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4def042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Optuna\n",
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "lgbm_study = optuna.create_study(\n",
    "    direction = 'maximize',\n",
    "    study_name = 'lgbm',\n",
    "    sampler = optuna.samplers.TPESampler(seed = 42)\n",
    ")\n",
    "lgbm_study.optimize(lgbm_objective, n_trials = 30, n_jobs = 3, show_progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8228f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna Result\n",
    "print('Best parameters:')\n",
    "print(lgbm_study.best_params)\n",
    "print('Best CV F1 Score:')\n",
    "print(lgbm_study.best_trial.value)\n",
    "\n",
    "# Save Best Parameters\n",
    "with open('lgbm_params.json', 'w') as f:\n",
    "    json.dump({f'model__{k}': v for k, v in lgbm_study.best_trial.params.items()}, f, indent = 4)\n",
    "\n",
    "# Save Best CV F1 Score\n",
    "with open('lgbm_cv_f1.txt', 'w') as f:\n",
    "    f.write(str(lgbm_study.best_trial.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb3fb2",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d77c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# RF\n",
    "# ==================================================\n",
    "\n",
    "# Define Pipeline\n",
    "classes, counts = np.unique(y, return_counts = True)\n",
    "rf = Pipeline([\n",
    "    ('prep', prep_tree),\n",
    "    ('model', BalancedRandomForestClassifier(\n",
    "        n_estimators = 100,\n",
    "        n_jobs = -1,\n",
    "        random_state = 42,\n",
    "        verbose = 1,\n",
    "        class_weight= {\n",
    "            0: counts.sum() / counts[0],\n",
    "            1: counts.sum() / counts[1]\n",
    "        }\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Parameter Grid\n",
    "rf_grid = {\n",
    "    'model__n_estimators': np.arange(100, 1501, 100)\n",
    "}\n",
    "\n",
    "rf_study = GridSearchCV(\n",
    "    rf,\n",
    "    param_grid = rf_grid,\n",
    "    scoring = 'f1',\n",
    "    n_jobs = -1,\n",
    "    cv = cv,\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0246f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GridSearchCV\n",
    "rf_study.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108af83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV Result\n",
    "print('Best parameter:')\n",
    "print(rf_study.best_params_)\n",
    "print('Best CV F1 Score:')\n",
    "print(rf_study.best_score_)\n",
    "\n",
    "# Save Best Parameters\n",
    "rf_params = rf_study.best_params_\n",
    "rf_params['model__n_estimators'] = int(rf_params['model__n_estimators'])\n",
    "with open('rf_params.json', 'w') as f:\n",
    "    json.dump(rf_study.best_params_, f, indent = 4)\n",
    "\n",
    "# Save Best CV F1 Score\n",
    "with open('rf_cv_f1.txt', 'w') as f:\n",
    "    f.write(str(rf_study.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c6b220",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ce8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# XGB\n",
    "# ==================================================\n",
    "\n",
    "# Define Pipeline\n",
    "xgb = Pipeline([\n",
    "    ('prep', prep_tree),\n",
    "    ('model', XGBClassifier(\n",
    "        verbosity = 1,\n",
    "        objective = 'binary:logistic',\n",
    "        eval_metric = 'aucpr',\n",
    "        scale_pos_weight = counts[0] / counts[1],\n",
    "        max_delta_step = 1,\n",
    "        random_state = 42,\n",
    "        tree_method = 'hist',\n",
    "        device = 'cuda'\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Optuna Objective\n",
    "def xgb_objective(trial):\n",
    "    params = {\n",
    "        'model__n_estimators': trial.suggest_int('n_estimators', 300, 2000),\n",
    "        'model__max_depth': trial.suggest_int('max_depth', 4, 16),\n",
    "        'model__min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "        'model__gamma': trial.suggest_float('gamma', 0, 10.0),\n",
    "        'model__learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.2, log = True),\n",
    "        'model__subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'model__colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'model__reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),\n",
    "        'model__reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0)\n",
    "    }\n",
    "\n",
    "    optuna_pipeline = clone(xgb).set_params(**params)\n",
    "\n",
    "    scores = cross_validate(\n",
    "        optuna_pipeline, x, y,\n",
    "        scoring = 'f1',\n",
    "        cv = cv,\n",
    "        n_jobs = 3,\n",
    "        verbose = 1\n",
    "    )\n",
    "    \n",
    "    return scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0151cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Optuna\n",
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "xgb_study = optuna.create_study(\n",
    "    direction = 'maximize',\n",
    "    study_name = 'xgb',\n",
    "    sampler = optuna.samplers.TPESampler(seed = 42)\n",
    ")\n",
    "xgb_study.optimize(xgb_objective, n_trials = 30, n_jobs = 3, show_progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df67287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna Result\n",
    "print('Best parameters:')\n",
    "print(xgb_study.best_params)\n",
    "print('Best CV F1 Score:')\n",
    "print(xgb_study.best_trial.value)\n",
    "\n",
    "# Save Best Parameters\n",
    "with open('xgb_params.json', 'w') as f:\n",
    "    json.dump({f'model__{k}': v for k, v in xgb_study.best_trial.params.items()}, f, indent = 4)\n",
    "\n",
    "# Save Best CV F1 Score\n",
    "with open('xgb_cv_f1.txt', 'w') as f:\n",
    "    f.write(str(xgb_study.best_trial.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d304685",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123704f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Ensemble\n",
    "# ==================================================\n",
    "\n",
    "# Refit LGBM\n",
    "with open('lgbm_params.json', 'r') as f:\n",
    "    lgbm_params = json.load(f)\n",
    "\n",
    "best_lgbm = lgbm.set_params(**lgbm_params)\n",
    "best_lgbm = best_lgbm.fit(x, y)\n",
    "\n",
    "# Refit RF\n",
    "with open('rf_params.json', 'r') as f:\n",
    "    rf_params = json.load(f)\n",
    "\n",
    "best_rf = rf.set_params(**rf_params)\n",
    "best_rf = best_rf.fit(x, y)\n",
    "\n",
    "# Refit XGB\n",
    "with open('xgb_params.json', 'r') as f:\n",
    "    xgb_params = json.load(f)\n",
    "\n",
    "best_xgb = xgb.set_params(**xgb_params)\n",
    "best_xgb = best_xgb.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51559001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta Features\n",
    "meta_x = pd.DataFrame({\n",
    "    'lgbm': best_lgbm.predict_proba(x)[:, 1],\n",
    "    'rf': best_rf.predict_proba(x)[:, 1],\n",
    "    'xgb': best_xgb.predict_proba(x)[:, 1],\n",
    "})\n",
    "\n",
    "# Optuna Objective\n",
    "def ensemble_objective(trial):\n",
    "    params = {\n",
    "        'threshold': trial.suggest_float('threshold', 0.2, 0.5)\n",
    "    }\n",
    "\n",
    "    probas = meta_x.sum(axis = 1)\n",
    "\n",
    "    preds = (probas >= 3 * params['threshold']).astype(int)\n",
    "    score = f1_score(y, preds)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e163f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Optuna\n",
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "ensemble_study = optuna.create_study(\n",
    "    direction = 'maximize',\n",
    "    study_name = 'ensemble',\n",
    "    sampler = optuna.samplers.TPESampler(seed = 42)\n",
    ")\n",
    "ensemble_study.optimize(ensemble_objective, n_trials = 200, n_jobs = -1, show_progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7b9c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna Result\n",
    "print('Best parameters:')\n",
    "print(ensemble_study.best_params)\n",
    "print('Best CV F1 Score:')\n",
    "print(ensemble_study.best_trial.value)\n",
    "\n",
    "# Save Best Parameters\n",
    "with open('ensemble_params.json', 'w') as f:\n",
    "    json.dump(ensemble_study.best_trial.params, f, indent = 4)\n",
    "\n",
    "# Save Best CV F1 Score\n",
    "with open('ensemble_cv_f1.txt', 'w') as f:\n",
    "    f.write(str(ensemble_study.best_trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a6ca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# Submission\n",
    "# ==================================================\n",
    "\n",
    "# Meta Features for Ensemble Model\n",
    "meta_x_test = pd.DataFrame({\n",
    "    'lgbm': best_lgbm.predict_proba(x_test)[:, 1],\n",
    "    'rf': best_rf.predict_proba(x_test)[:, 1],\n",
    "    'xgb': best_xgb.predict_proba(x_test)[:, 1]\n",
    "})\n",
    "\n",
    "# Prediction\n",
    "with open('ensemble_params.json', 'r') as f:\n",
    "    ensemble_params = json.load(f)\n",
    "probas = meta_x_test.sum(axis = 1)\n",
    "preds = (probas >= 3 * ensemble_params['threshold']).astype(int)\n",
    "\n",
    "# Submission\n",
    "submission = pd.read_csv('test.csv')\n",
    "submission['Cancer'] = preds\n",
    "submission = submission.copy()[['ID', 'Cancer']]\n",
    "submission.to_csv('submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
